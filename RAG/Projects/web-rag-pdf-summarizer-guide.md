# Web App RAG PDF Summarizer - Build Guide

A step-by-step guide to build a web-based RAG (Retrieval-Augmented Generation) PDF summarizer with chat interface.

## üéØ Project Overview

**What you'll build:**
- Frontend: React with modern UI (Tailwind CSS)
- Backend: FastAPI (Python)
- RAG Pipeline: LangChain + ChromaDB + HuggingFace Embeddings
- LLM: Ollama (local) with Gemma or Llama models
- Features: PDF upload, document chunking, Q&A with memory, chat history

**Tech Stack:**
- **Frontend:** React, Vite, Tailwind CSS, Axios
- **Backend:** FastAPI, Python 3.10+
- **RAG:** LangChain, ChromaDB, HuggingFace
- **LLM:** Ollama (Gemma/Llama)

---

## üìã Prerequisites

### Required Software
```bash
# Python 3.10+
python --version

# Node.js 18+
node --version

# Ollama (for local LLM)
# Download from: https://ollama.ai
ollama --version

# Pull a model
ollama pull gemma2:2b
# or
ollama pull llama3.2:3b
```

### Required Python Packages
```bash
pip install fastapi uvicorn python-multipart
pip install langchain langchain-community
pip install chromadb sentence-transformers
pip install PyPDF2 pypdf
pip install python-dotenv
```

---

## üèóÔ∏è Project Structure

```
web-rag-summarizer/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                 # FastAPI main app
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine.py          # RAG logic
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ uploads/               # Temporary PDF storage
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/             # Vector database
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileUpload.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInterface.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ModeSelector.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ client.js
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ tailwind.config.js
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Step-by-Step Build Guide

### **STEP 1: Project Initialization**

#### 1.1 Create Project Folders
```bash
mkdir web-rag-summarizer
cd web-rag-summarizer
mkdir backend frontend
```

#### 1.2 Initialize Backend
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Create requirements.txt
cat > requirements.txt << EOL
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
langchain==0.1.0
langchain-community==0.0.10
chromadb==0.4.22
sentence-transformers==2.2.2
PyPDF2==3.0.1
pypdf==3.17.4
python-dotenv==1.0.0
pydantic==2.5.2
EOL

pip install -r requirements.txt
```

#### 1.3 Initialize Frontend
```bash
cd ../frontend
npm create vite@latest . -- --template react
npm install
npm install axios tailwindcss postcss autoprefixer
npm install lucide-react
npx tailwindcss init -p
```

---

### **STEP 2: Backend Development**

#### 2.1 Create RAG Engine (`backend/rag_engine.py`)
```python
import os
from typing import List, Dict
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_models import ChatOllama
from langchain.prompts import PromptTemplate

class RAGEngine:
    def __init__(self, model_name="gemma2:2b", persist_directory="./chroma_db"):
        self.model_name = model_name
        self.persist_directory = persist_directory
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.vectorstore = None
        self.llm = None
        self.memory = None
        self.qa_chain = None
        self._initialize_llm()
        
    def _initialize_llm(self):
        """Initialize the local LLM with Ollama"""
        self.llm = ChatOllama(
            model=self.model_name,
            temperature=0.7,
            num_predict=512
        )
    
    def load_and_process_pdf(self, pdf_path: str) -> bool:
        """Load PDF, split into chunks, and create embeddings"""
        try:
            # Load PDF
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            
            # Split documents
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )
            chunks = text_splitter.split_documents(documents)
            
            # Create vector store
            self.vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            
            # Initialize memory for conversation
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            
            # Create QA chain
            self._create_qa_chain()
            
            return True
        except Exception as e:
            print(f"Error processing PDF: {e}")
            return False
    
    def _create_qa_chain(self):
        """Create the conversational retrieval chain"""
        prompt_template = """Use the following pieces of context to answer the question at the end. 
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}

Question: {question}

Answer in a helpful and detailed way:"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": PROMPT}
        )
    
    def query(self, question: str) -> Dict:
        """Query the RAG system"""
        if not self.qa_chain:
            return {
                "answer": "Please upload a PDF first.",
                "sources": []
            }
        
        try:
            response = self.qa_chain({"question": question})
            
            return {
                "answer": response["answer"],
                "sources": [
                    {
                        "page": doc.metadata.get("page", 0),
                        "content": doc.page_content[:200]
                    }
                    for doc in response.get("source_documents", [])
                ]
            }
        except Exception as e:
            return {
                "answer": f"Error processing query: {str(e)}",
                "sources": []
            }
    
    def reset_memory(self):
        """Clear conversation memory"""
        if self.memory:
            self.memory.clear()
```

#### 2.2 Create Pydantic Models (`backend/models.py`)
```python
from pydantic import BaseModel
from typing import List, Optional

class QueryRequest(BaseModel):
    question: str

class SourceDocument(BaseModel):
    page: int
    content: str

class QueryResponse(BaseModel):
    answer: str
    sources: List[SourceDocument]

class StatusResponse(BaseModel):
    status: str
    message: str
```

#### 2.3 Create FastAPI App (`backend/app.py`)
```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import os
import shutil
from pathlib import Path

from rag_engine import RAGEngine
from models import QueryRequest, QueryResponse, StatusResponse

app = FastAPI(title="RAG PDF Summarizer API")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # Vite default port
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global RAG engine instance
rag_engine = None
UPLOAD_DIR = Path("./uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

@app.get("/")
async def root():
    return {"message": "RAG PDF Summarizer API is running"}

@app.post("/upload", response_model=StatusResponse)
async def upload_pdf(file: UploadFile = File(...)):
    """Upload and process a PDF file"""
    global rag_engine
    
    # Validate file type
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are allowed")
    
    try:
        # Save uploaded file
        file_path = UPLOAD_DIR / file.filename
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Initialize RAG engine if not exists
        if rag_engine is None:
            rag_engine = RAGEngine()
        
        # Process PDF
        success = rag_engine.load_and_process_pdf(str(file_path))
        
        if success:
            return StatusResponse(
                status="success",
                message=f"PDF '{file.filename}' processed successfully"
            )
        else:
            raise HTTPException(status_code=500, detail="Failed to process PDF")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        file.file.close()

@app.post("/query", response_model=QueryResponse)
async def query_pdf(request: QueryRequest):
    """Query the uploaded PDF"""
    global rag_engine
    
    if rag_engine is None:
        raise HTTPException(status_code=400, detail="Please upload a PDF first")
    
    try:
        result = rag_engine.query(request.question)
        return QueryResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset", response_model=StatusResponse)
async def reset_conversation():
    """Reset conversation memory"""
    global rag_engine
    
    if rag_engine:
        rag_engine.reset_memory()
    
    return StatusResponse(
        status="success",
        message="Conversation memory cleared"
    )

@app.delete("/clear", response_model=StatusResponse)
async def clear_all():
    """Clear all data and reset"""
    global rag_engine
    
    rag_engine = None
    
    # Clean up uploads
    for file in UPLOAD_DIR.glob("*"):
        file.unlink()
    
    return StatusResponse(
        status="success",
        message="All data cleared"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

### **STEP 3: Frontend Development**

#### 3.1 Configure Tailwind (`frontend/tailwind.config.js`)
```javascript
/** @type {import('tailwindcss').Config} */
export default {
  content: [
    "./index.html",
    "./src/**/*.{js,ts,jsx,tsx}",
  ],
  theme: {
    extend: {},
  },
  plugins: [],
}
```

#### 3.2 Update CSS (`frontend/src/index.css`)
```css
@tailwind base;
@tailwind components;
@tailwind utilities;

body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
```

#### 3.3 Create API Client (`frontend/src/api/client.js`)
```javascript
import axios from 'axios';

const API_BASE_URL = 'http://localhost:8000';

const apiClient = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    'Content-Type': 'application/json',
  },
});

export const uploadPDF = async (file) => {
  const formData = new FormData();
  formData.append('file', file);
  
  const response = await axios.post(`${API_BASE_URL}/upload`, formData, {
    headers: {
      'Content-Type': 'multipart/form-data',
    },
  });
  
  return response.data;
};

export const queryPDF = async (question) => {
  const response = await apiClient.post('/query', { question });
  return response.data;
};

export const resetConversation = async () => {
  const response = await apiClient.post('/reset');
  return response.data;
};

export const clearAll = async () => {
  const response = await apiClient.delete('/clear');
  return response.data;
};
```

#### 3.4 Create FileUpload Component (`frontend/src/components/FileUpload.jsx`)
```jsx
import React, { useState } from 'react';
import { Upload, FileText, CheckCircle, AlertCircle } from 'lucide-react';
import { uploadPDF } from '../api/client';

const FileUpload = ({ onUploadSuccess }) => {
  const [file, setFile] = useState(null);
  const [uploading, setUploading] = useState(false);
  const [status, setStatus] = useState({ type: '', message: '' });

  const handleFileChange = (e) => {
    const selectedFile = e.target.files[0];
    if (selectedFile && selectedFile.type === 'application/pdf') {
      setFile(selectedFile);
      setStatus({ type: '', message: '' });
    } else {
      setStatus({ type: 'error', message: 'Please select a valid PDF file' });
    }
  };

  const handleUpload = async () => {
    if (!file) return;

    setUploading(true);
    setStatus({ type: '', message: '' });

    try {
      const response = await uploadPDF(file);
      setStatus({ type: 'success', message: response.message });
      onUploadSuccess();
    } catch (error) {
      setStatus({
        type: 'error',
        message: error.response?.data?.detail || 'Failed to upload PDF',
      });
    } finally {
      setUploading(false);
    }
  };

  return (
    <div className="bg-white rounded-lg shadow-md p-6 mb-6">
      <h2 className="text-2xl font-bold mb-4 flex items-center gap-2">
        <Upload className="w-6 h-6" />
        Upload PDF Document
      </h2>

      <div className="border-2 border-dashed border-gray-300 rounded-lg p-8 text-center">
        <input
          type="file"
          accept=".pdf"
          onChange={handleFileChange}
          className="hidden"
          id="file-upload"
        />
        <label
          htmlFor="file-upload"
          className="cursor-pointer flex flex-col items-center"
        >
          <FileText className="w-16 h-16 text-gray-400 mb-4" />
          <span className="text-lg font-medium text-gray-700">
            {file ? file.name : 'Choose a PDF file'}
          </span>
          <span className="text-sm text-gray-500 mt-2">
            Click to browse or drag and drop
          </span>
        </label>
      </div>

      {file && (
        <button
          onClick={handleUpload}
          disabled={uploading}
          className="mt-4 w-full bg-blue-600 text-white py-3 rounded-lg hover:bg-blue-700 disabled:bg-gray-400 transition-colors font-medium"
        >
          {uploading ? 'Processing...' : 'Upload and Process'}
        </button>
      )}

      {status.message && (
        <div
          className={`mt-4 p-4 rounded-lg flex items-center gap-2 ${
            status.type === 'success'
              ? 'bg-green-50 text-green-800'
              : 'bg-red-50 text-red-800'
          }`}
        >
          {status.type === 'success' ? (
            <CheckCircle className="w-5 h-5" />
          ) : (
            <AlertCircle className="w-5 h-5" />
          )}
          <span>{status.message}</span>
        </div>
      )}
    </div>
  );
};

export default FileUpload;
```

#### 3.5 Create ChatInterface Component (`frontend/src/components/ChatInterface.jsx`)
```jsx
import React, { useState, useRef, useEffect } from 'react';
import { Send, Bot, User, Loader } from 'lucide-react';
import { queryPDF } from '../api/client';

const ChatInterface = () => {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const messagesEndRef = useRef(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const handleSubmit = async (e) => {
    e.preventDefault();
    if (!input.trim() || loading) return;

    const userMessage = { role: 'user', content: input };
    setMessages((prev) => [...prev, userMessage]);
    setInput('');
    setLoading(true);

    try {
      const response = await queryPDF(input);
      const assistantMessage = {
        role: 'assistant',
        content: response.answer,
        sources: response.sources,
      };
      setMessages((prev) => [...prev, assistantMessage]);
    } catch (error) {
      const errorMessage = {
        role: 'assistant',
        content: 'Sorry, I encountered an error processing your question.',
        error: true,
      };
      setMessages((prev) => [...prev, errorMessage]);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="bg-white rounded-lg shadow-md flex flex-col h-[600px]">
      <div className="p-4 border-b">
        <h2 className="text-2xl font-bold flex items-center gap-2">
          <Bot className="w-6 h-6" />
          Chat with your PDF
        </h2>
      </div>

      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.length === 0 && (
          <div className="text-center text-gray-500 mt-8">
            <p className="text-lg">Ask a question about your document</p>
            <p className="text-sm mt-2">
              Try: "What is this document about?" or "Summarize the main points"
            </p>
          </div>
        )}

        {messages.map((message, index) => (
          <div
            key={index}
            className={`flex gap-3 ${
              message.role === 'user' ? 'justify-end' : 'justify-start'
            }`}
          >
            {message.role === 'assistant' && (
              <div className="w-8 h-8 rounded-full bg-blue-600 flex items-center justify-center flex-shrink-0">
                <Bot className="w-5 h-5 text-white" />
              </div>
            )}

            <div
              className={`max-w-[70%] rounded-lg p-4 ${
                message.role === 'user'
                  ? 'bg-blue-600 text-white'
                  : message.error
                  ? 'bg-red-50 text-red-800'
                  : 'bg-gray-100 text-gray-800'
              }`}
            >
              <p className="whitespace-pre-wrap">{message.content}</p>
              
              {message.sources && message.sources.length > 0 && (
                <div className="mt-3 pt-3 border-t border-gray-300">
                  <p className="text-xs font-semibold mb-2">Sources:</p>
                  {message.sources.map((source, idx) => (
                    <div key={idx} className="text-xs mb-1">
                      <span className="font-medium">Page {source.page + 1}:</span>{' '}
                      {source.content.substring(0, 100)}...
                    </div>
                  ))}
                </div>
              )}
            </div>

            {message.role === 'user' && (
              <div className="w-8 h-8 rounded-full bg-gray-600 flex items-center justify-center flex-shrink-0">
                <User className="w-5 h-5 text-white" />
              </div>
            )}
          </div>
        ))}

        {loading && (
          <div className="flex gap-3 justify-start">
            <div className="w-8 h-8 rounded-full bg-blue-600 flex items-center justify-center">
              <Bot className="w-5 h-5 text-white" />
            </div>
            <div className="bg-gray-100 rounded-lg p-4">
              <Loader className="w-5 h-5 animate-spin" />
            </div>
          </div>
        )}

        <div ref={messagesEndRef} />
      </div>

      <form onSubmit={handleSubmit} className="p-4 border-t">
        <div className="flex gap-2">
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            placeholder="Ask a question..."
            className="flex-1 px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-600"
            disabled={loading}
          />
          <button
            type="submit"
            disabled={loading || !input.trim()}
            className="bg-blue-600 text-white p-2 rounded-lg hover:bg-blue-700 disabled:bg-gray-400 transition-colors"
          >
            <Send className="w-5 h-5" />
          </button>
        </div>
      </form>
    </div>
  );
};

export default ChatInterface;
```

#### 3.6 Create Main App Component (`frontend/src/App.jsx`)
```jsx
import React, { useState } from 'react';
import FileUpload from './components/FileUpload';
import ChatInterface from './components/ChatInterface';
import { resetConversation, clearAll } from './api/client';
import { RefreshCw, Trash2 } from 'lucide-react';

function App() {
  const [pdfUploaded, setPdfUploaded] = useState(false);

  const handleUploadSuccess = () => {
    setPdfUploaded(true);
  };

  const handleReset = async () => {
    try {
      await resetConversation();
      alert('Conversation reset successfully');
      window.location.reload();
    } catch (error) {
      alert('Failed to reset conversation');
    }
  };

  const handleClearAll = async () => {
    if (confirm('Are you sure you want to clear all data?')) {
      try {
        await clearAll();
        setPdfUploaded(false);
        alert('All data cleared successfully');
        window.location.reload();
      } catch (error) {
        alert('Failed to clear data');
      }
    }
  };

  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100">
      <div className="container mx-auto px-4 py-8 max-w-6xl">
        <div className="text-center mb-8">
          <h1 className="text-4xl font-bold text-gray-800 mb-2">
            RAG PDF Summarizer
          </h1>
          <p className="text-gray-600">
            Upload a PDF and ask questions using AI
          </p>
        </div>

        <div className="mb-4 flex gap-4 justify-end">
          <button
            onClick={handleReset}
            className="flex items-center gap-2 px-4 py-2 bg-yellow-500 text-white rounded-lg hover:bg-yellow-600 transition-colors"
          >
            <RefreshCw className="w-4 h-4" />
            Reset Conversation
          </button>
          <button
            onClick={handleClearAll}
            className="flex items-center gap-2 px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 transition-colors"
          >
            <Trash2 className="w-4 h-4" />
            Clear All
          </button>
        </div>

        <FileUpload onUploadSuccess={handleUploadSuccess} />

        {pdfUploaded && <ChatInterface />}
      </div>
    </div>
  );
}

export default App;
```

---

### **STEP 4: Running the Application**

#### 4.1 Start Backend Server
```bash
cd backend
source venv/bin/activate  # On Windows: venv\Scripts\activate
python app.py

# Should see:
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

#### 4.2 Start Frontend Development Server
```bash
cd frontend
npm run dev

# Should see:
# VITE v5.x.x  ready in xxx ms
# ‚ûú  Local:   http://localhost:5173/
```

#### 4.3 Verify Ollama is Running
```bash
ollama list
ollama serve  # If not already running
```

---

## üß™ Testing the Application

### Test Checklist:
1. ‚úÖ Open browser to `http://localhost:5173`
2. ‚úÖ Upload a PDF file
3. ‚úÖ Wait for processing confirmation
4. ‚úÖ Ask a question: "What is this document about?"
5. ‚úÖ Verify response with sources
6. ‚úÖ Ask follow-up questions (memory test)
7. ‚úÖ Click "Reset Conversation"
8. ‚úÖ Click "Clear All" to start fresh

---

## üöÄ Enhancement Ideas

### Phase 2 Features:
1. **Multiple PDF Support**: Upload and query multiple documents
2. **Document Management**: List, view, and delete uploaded PDFs
3. **Export Chat History**: Download conversations as JSON/PDF
4. **Advanced Settings**: Adjust temperature, chunk size, model selection
5. **User Authentication**: Add login/signup with JWT
6. **Streaming Responses**: Real-time token streaming for answers
7. **Syntax Highlighting**: Code snippets in responses
8. **Dark Mode**: Toggle dark/light theme
9. **Mobile Responsive**: Optimize for mobile devices
10. **Docker Deployment**: Containerize the entire app

### Phase 3 (Advanced):
- **Vector Database Comparison**: Test FAISS, Pinecone, Weaviate
- **Multiple LLM Support**: GPT-4, Claude, Mistral
- **Citation Generation**: Proper academic citations
- **OCR Support**: Handle scanned PDFs
- **Multi-language**: Support non-English documents
- **Analytics Dashboard**: Track usage, popular queries

---

## üêõ Troubleshooting

### Common Issues:

**1. CORS Error**
```python
# Ensure backend app.py has correct origin:
allow_origins=["http://localhost:5173"]
```

**2. Ollama Connection Error**
```bash
# Check if Ollama is running
ollama list
# Restart if needed
ollama serve
```

**3. PDF Processing Fails**
```bash
# Verify PyPDF2 installation
pip install --upgrade PyPDF2 pypdf
```

**4. Memory Issues with Large PDFs**
```python
# Reduce chunk size in rag_engine.py
chunk_size=500,  # Instead of 1000
```

**5. Slow Response Times**
```python
# Use smaller model
ollama pull gemma2:2b
# or
model_name="gemma2:2b"
```

---

## üìö Learning Resources

### Key Concepts to Study:
1. **RAG Architecture**: Retrieval-Augmented Generation
2. **Vector Embeddings**: How text is converted to numbers
3. **ChromaDB**: Vector database operations
4. **LangChain**: Chaining LLM operations
5. **FastAPI**: Async Python web framework
6. **React Hooks**: useState, useEffect, useRef
7. **Tailwind CSS**: Utility-first CSS framework

### Recommended Reading:
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Guide](https://docs.trychroma.com/)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)
- [React Documentation](https://react.dev/)
- [RAG Pattern Explanation](https://aws.amazon.com/what-is/retrieval-augmented-generation/)

---

## üéØ Learning Path

### Week 1: Backend Focus
- Day 1-2: Understand RAG pipeline
- Day 3-4: Build RAG engine
- Day 5-6: Create FastAPI endpoints
- Day 7: Test with Postman/cURL

### Week 2: Frontend Focus
- Day 1-2: React basics + file upload
- Day 3-4: Chat interface + state management
- Day 5-6: UI polish + Tailwind
- Day 7: Integration testing

### Week 3: Polish
- Day 1-2: Error handling
- Day 3-4: Performance optimization
- Day 5-6: Additional features
- Day 7: Documentation + deployment prep

---

## üìù Project Variations

### Easier Version (Start Here):
- Remove memory feature
- Use simple text input instead of PDF
- Basic styling without Tailwind
- Single question/answer mode

### Harder Version (Challenge):
- Add PostgreSQL for user management
- Implement websockets for streaming
- Deploy to AWS/Vercel
- Add comprehensive testing (pytest, Jest)

---

## ‚úÖ Success Criteria

You've successfully completed this project when:
- ‚úÖ You can upload any PDF
- ‚úÖ The system chunks and embeds the content
- ‚úÖ You can ask questions and get relevant answers
- ‚úÖ Sources are cited with page numbers
- ‚úÖ Conversation maintains context (memory)
- ‚úÖ UI is clean and responsive
- ‚úÖ Error handling works properly

---

## üéì What You'll Learn

**Webdev Skills:**
- Full-stack application architecture
- REST API design and implementation
- React component composition
- State management in React
- Async operations (promises, async/await)
- File upload handling
- CORS and security basics
- Modern CSS with Tailwind

**RAG/AI Skills:**
- Document processing and chunking
- Vector embeddings and similarity search
- Retrieval-Augmented Generation pattern
- Local LLM usage with Ollama
- Prompt engineering
- Conversation memory management
- LangChain framework

**DevOps/Tools:**
- Virtual environments (Python)
- Package management (pip, npm)
- API testing and debugging
- Version control practices
- Development workflow

---

## üí° Tips for Success

1. **Start Small**: Get basic upload/query working first
2. **Test Often**: Use Postman to test backend before frontend
3. **Read Errors**: Console logs are your friend
4. **Use Git**: Commit after each working feature
5. **Ask Questions**: Use ChatGPT/Claude for debugging
6. **Iterate**: First make it work, then make it pretty
7. **Document**: Comment your code as you write

---

## üîó Next Steps After Completion

1. Deploy to production (Render, Railway, Vercel)
2. Add to your portfolio with live demo
3. Write a blog post about building it
4. Contribute RAG features to open-source projects
5. Build v2 with advanced features
6. Try other RAG use cases (code assistant, legal docs, etc.)

---

**Happy Building! üöÄ**

Remember: The goal is to learn by doing. Don't worry about perfection on your first attempt. Build, break, fix, repeat!
