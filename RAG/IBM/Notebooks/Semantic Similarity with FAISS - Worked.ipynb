{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca6f05f-9602-40e7-9989-ee399ae5f51e",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6587c2-b063-418d-afad-6a7dc49f7bb3",
   "metadata": {},
   "source": [
    "# Semantic Similarity with FAISS\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "Welcome to a hands-on exploration of semantic search, where we unravel the intricacies of finding meaning in text. This lab is a beginner's journey into the realm of advanced information retrieval. You'll start by learning the essentials of text preprocessing to enhance data quality. Next, you'll dive into the world of vector spaces, using the Universal Sentence Encoder to convert text into a format that machines understand. Finally, you'll harness the efficiency of FAISS, a library built for rapid similarity search, to compare and retrieve information. By the end of our session, you'll have a functional semantic search engine that not only understands the subtleties of human language but also fetches information that truly matters.\n",
    "\n",
    "<p style='color: red'>Embark on this learning adventure to build a search engine that sees beyond the obvious, leveraging context and semantics to satisfy the quest for information.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8f365-a6ec-4cfa-9e20-a3ed68b5ac28",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Understanding-Semantic-Search\">Understanding Semantic Search</a>\n",
    "    </li>\n",
    "    <li><a href=\"#Understanding-Vectorization-and-Indexing\">Understanding Vectorization and Indexing</a></li>\n",
    "    <li><a href=\"#The-20-Newsgroups-Dataset\">The 20 Newsgroups Dataset</a></li>\n",
    "    <li><a href=\"#Pre-processing-Data\">Pre-processing Data</a></li>\n",
    "    <li><a href=\"#Universal-Sentence-Encoder\">Universal Sentence Encoder</a></li>\n",
    "    <li><a href=\"#Indexing-with-FAISS\">Indexing with FAISS</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50057af-abfe-428a-a054-15a4abc10078",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa8f58-43d5-4107-a821-cc1425629fe0",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "In this lab, our objectives are to:\n",
    "\n",
    "- Understand the fundamentals of semantic search and its advantages over traditional search methods.\n",
    "- Familiarize with the process of preparing text data for semantic analysis, including cleaning and standardization techniques.\n",
    "- Learn how to utilize the Universal Sentence Encoder to convert text into high-dimensional vector space representations.\n",
    "- Gain practical experience with FAISS (Facebook AI Similarity Search), an efficient library for indexing and searching high-dimensional vectors.\n",
    "- Apply these techniques to build a fully functioning semantic search engine that can interpret and respond to natural language queries.\n",
    "\n",
    "By accomplishing these objectives, you will acquire a comprehensive skill set that underpins advanced search functionalities in modern AI-driven systems, preparing you for further exploration and development in the field of natural language processing and information retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb269ef4-2e7a-47af-95b6-811aa6bce449",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ea841-da4e-4d6b-b2e8-6dfe1c4cd1fd",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To ensure a smooth experience throughout this lab, we need to set up our environment properly. This includes installing necessary libraries, importing them, and preparing helper functions that will be used later in the lab.\n",
    "\n",
    "## Installing Required Libraries\n",
    "\n",
    "Before we start, you need to install the following libraries if you haven't already:\n",
    "\n",
    "- `tensorflow`: The core library for TensorFlow, required for working with the Universal Sentence Encoder.\n",
    "- `tensorflow-hub`: A library that makes it easy to download and deploy pre-trained TensorFlow models, including the Universal Sentence Encoder.\n",
    "- `faiss-cpu`: A library for efficient similarity search and clustering of dense vectors.\n",
    "- `numpy`: A library for numerical computing, which we will use to handle arrays and matrices.\n",
    "- `scikit-learn`: A machine learning library that provides various tools for data mining and data analysis, useful for additional tasks like data splitting and evaluation metrics.\n",
    "\n",
    "You can install these libraries using `pip` with the following commands:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fe435-9a6e-4b41-bfad-f228376e84e4",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20913cb8-c6fc-46ad-8258-4c5ca529e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-macosx_14_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.4.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-macosx_14_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.0-cp311-cp311-macosx_14_0_arm64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl (8.1 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached scipy-1.16.3-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, faiss-cpu, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed faiss-cpu-1.13.2 joblib-1.5.3 numpy-2.4.0 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "Collecting tensorflow>=2.0.0\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.0.0)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.0.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.0.0)\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.0.0)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow>=2.0.0)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.0.0)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow>=2.0.0)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow>=2.0.0) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow>=2.0.0)\n",
      "  Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow>=2.0.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow>=2.0.0) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow>=2.0.0) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.0.0)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow>=2.0.0) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow>=2.0.0)\n",
      "  Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.0.0)\n",
      "  Using cached grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/tensorboard/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorboard~=2.20.0 (from tensorflow>=2.0.0)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/9c/d9/a5db55f88f258ac669a92858b70a714bbbd5acd993820b41ec4a96a4d77f/tensorboard-2.20.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/keras/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting keras>=3.10.0 (from tensorflow>=2.0.0)\n",
      "  Downloading keras-3.13.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow>=2.0.0) (2.4.0)\n",
      "Collecting h5py>=3.11.0 (from tensorflow>=2.0.0)\n",
      "  Downloading h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.0.0)\n",
      "  Downloading ml_dtypes-0.5.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (8.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow>=2.0.0)\n",
      "  Using cached charset_normalizer-3.4.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow>=2.0.0)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow>=2.0.0)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow>=2.0.0)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow>=2.0.0)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pillow (from tensorboard~=2.20.0->tensorflow>=2.0.0)\n",
      "  Using cached pillow-12.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow>=2.0.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow>=2.0.0)\n",
      "  Using cached werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=2.0.0)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow>=2.0.0)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow>=2.0.0)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow>=2.0.0)\n",
      "  Downloading optree-0.18.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting markupsafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.0.0)\n",
      "  Using cached markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow>=2.0.0)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow>=2.0.0) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.0.0)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/ef/69/de33bd90dbddc8eede8f99ddeccfb374f7e18f84beb404bfe2cbbdf8df90/tensorflow-2.20.0-cp311-cp311-macosx_12_0_arm64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading tensorflow-2.20.0-cp311-cp311-macosx_12_0_arm64.whl (200.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 MB\u001b[0m \u001b[31m229.9 kB/s\u001b[0m  \u001b[33m0:03:31\u001b[0m0:00:02\u001b[0m00:42\u001b[0m\n",
      "\u001b[?25hUsing cached grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl (11.8 MB)\n",
      "Downloading ml_dtypes-0.5.4-cp311-cp311-macosx_10_9_universal2.whl (679 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.7/679.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp311-cp311-macosx_10_9_universal2.whl (206 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m886.1 kB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m586.3 kB/s\u001b[0m  \u001b[33m0:00:36\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "Using cached markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp311-cp311-macosx_11_0_arm64.whl (337 kB)\n",
      "Using cached pillow-12.0.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorboard-data-server, protobuf, pillow, optree, opt_einsum, ml_dtypes, mdurl, markupsafe, markdown, idna, h5py, grpcio, google_pasta, gast, charset_normalizer, certifi, absl-py, werkzeug, requests, markdown-it-py, astunparse, tensorboard, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [tensorflow]2\u001b[0m [tensorflow]]py]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 certifi-2025.11.12 charset_normalizer-3.4.4 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 idna-3.11 keras-3.13.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 pillow-12.0.0 protobuf-6.33.2 requests-2.32.5 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 urllib3-2.6.2 werkzeug-3.1.4 wheel-0.45.1 wrapt-2.0.1\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow-hub) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow-hub) (6.33.2)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (25.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.13.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.10)\n",
      "Requirement already satisfied: pillow in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.1.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (14.2.0)\n",
      "Requirement already satisfied: namex in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/markdanielcallejas/Desktop/Main/study-notes-vault/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras, tensorflow-hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [tensorflow-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tensorflow-hub-0.16.1 tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu numpy scikit-learn\n",
    "!pip install \"tensorflow>=2.0.0\"\n",
    "!pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671aa3e-b469-4792-a360-e2debb4d2236",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be498aa-7902-45e9-914e-5587372d3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# Suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec24e6-0731-43d5-8feb-73e5a5bed524",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933298c-2555-4759-a847-d08eb8122e43",
   "metadata": {},
   "source": [
    "## Understanding Semantic Search\n",
    "\n",
    "When we're looking to build a semantic search engine, it's important to start with the basics. Let's break down what semantic search is and why it's a game-changer in finding information.\n",
    "\n",
    "### What is Semantic Search?\n",
    "\n",
    "Semantic search transcends the limitations of traditional keyword searches by understanding the context and nuances of language in user queries. At its core, semantic search:\n",
    "\n",
    "- Enhances the search experience by interpreting the intent and contextual meaning behind search queries.\n",
    "- Delivers more accurate and relevant search results by analyzing the relationships between words and phrases within the search context.\n",
    "- Adapts to user behavior and preferences, refining search results for better user satisfaction.\n",
    "\n",
    "### How Semantic Search Works - The Simple Version\n",
    "\n",
    "Now, how does this smart assistant do its job? It uses some clever tricks from a field called Natural Language Processing, or NLP for short. Here’s the simple version of the process:\n",
    "\n",
    "- **Getting the Gist**: First up, the search engine listens to your query and tries to get the gist of it. Instead of just spotting keywords, it digs deeper to find the real meaning.\n",
    "- **Making Connections**: Next, it thinks about all the different ways words can be related (like \"doctor\" and \"physician\" meaning the same thing). This helps it get a better sense of what you're asking for.\n",
    "- **Picking the Best**: Finally, it acts like a librarian who knows every book in the library. It sorts through tons of information to pick what matches your query best, considering what you probably mean.\n",
    "\n",
    "### The Technical Side of Semantic Search\n",
    "\n",
    "After understanding the basics, let's peek under the hood at the technical engine powering semantic search. This part is a bit like math class, where we learn about vectors — no, not the ones you learned in physics, but something similar that we use in search engines.\n",
    "\n",
    "#### Vectors: The Language of Semantic Search\n",
    "\n",
    "In the world of semantic search, a vector is a list of numbers that a computer uses to represent the meaning of words or sentences. Imagine each word or sentence as a point in space. The closer two points are, the more similar their meanings.\n",
    "\n",
    "- **Creating Vectors**: We start by turning words or sentences into vectors using models like the Universal Sentence Encoder. It's like giving each piece of text its unique numerical fingerprint.\n",
    "- **Calculating Similarity**: To find out how similar two pieces of text are, we measure how close their vectors are in space. This is done using mathematical formulas, such as cosine similarity, which tells us how similar or different two text fingerprints are.\n",
    "- **Using Vectors for Search**: When you search for something, the search engine looks for the vectors closest to the vector of your query. The closest vectors represent the most relevant results to what you're asking.\n",
    "\n",
    "#### How Vectors Power Our Search\n",
    "\n",
    "Vectors are powerful because they can capture the subtle meanings of language that go beyond the surface of words. Here's what happens in a semantic search engine:\n",
    "\n",
    "1. **Vectorization**: When we type in a search query, the engine immediately turns our words into a vector.\n",
    "2. **Indexing**: It then quickly scans through a massive index of other vectors, each representing different pieces of information.\n",
    "3. **Retrieval**: By finding the closest matching vectors, the engine retrieves information that's not just textually similar but semantically related.\n",
    "\n",
    "By the end of this guide, you'll understand how to create a search engine that does all of this and more. We'll start simple and build up step by step. Ready? Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0baf93-892a-431f-adcc-a272029c4589",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16c4f9-7fc7-4919-8c3b-2311942aa4ef",
   "metadata": {},
   "source": [
    "## Understanding Vectorization and Indexing\n",
    "\n",
    "Vectorization and indexing are key components of building a semantic search engine. Let's explore how they work using the Universal Sentence Encoder (USE) and FAISS.\n",
    "\n",
    "### What does the Universal Sentence Encoder do?\n",
    "\n",
    "The Universal Sentence Encoder (USE) takes sentences, no matter how complex, and turns them into vectors. These vectors are arrays of numbers that capture the essence of sentences. Here's why it's amazing:\n",
    "\n",
    "- **Language Comprehension**: USE understands the meaning of sentences by considering the context in which each word is used.\n",
    "- **Versatility**: It's trained on a variety of data sources, enabling it to handle a wide range of topics and sentence structures.\n",
    "- **Speed**: Once trained, USE can quickly convert sentences to vectors, making it highly efficient.\n",
    "\n",
    "### How does the Universal Sentence Encoder work?\n",
    "\n",
    "The magic of USE lies in its training. It uses deep learning models to digest vast amounts of text. Here’s what it does:\n",
    "\n",
    "1. **Analyzes Words**: It looks at each word in a sentence and the words around it to get a full picture of their meaning.\n",
    "2. **Understands Context**: It pays attention to the order of words and how they're used together to grasp the sentence's intent.\n",
    "3. **Creates Vectors**: It converts all this understanding into a numeric vector that represents the sentence.\n",
    "\n",
    "### What is FAISS and what does it do?\n",
    "\n",
    "FAISS, developed by Facebook AI, is a library for efficient similarity search. After we have vectors from USE, we need a way to search through them quickly to find the most relevant ones to a query. FAISS does just that:\n",
    "\n",
    "- **Efficient Searching**: It uses optimized algorithms to rapidly search through large collections of vectors.\n",
    "- **Scalability**: It can handle databases of vectors that are too large to fit in memory, making it suitable for big data applications.\n",
    "- **Accuracy**: It provides highly accurate search results, thanks to its advanced indexing strategies.\n",
    "\n",
    "### How does FAISS work?\n",
    "\n",
    "FAISS creates an index of all the vectors, which allows it to search through them efficiently. Here's a simplified version of its process:\n",
    "\n",
    "1. **Index Building**: It organizes vectors in a way that similar ones are near each other, making it faster to find matches.\n",
    "2. **Searching**: When you search with a new vector, FAISS quickly identifies which part of the index to look at for the closest matches.\n",
    "3. **Retrieving Results**: It then retrieves the most similar vectors, which correspond to the most relevant search results.\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "With USE and FAISS, we have a powerful duo. USE helps us understand language in numerical terms, and FAISS lets us search through these numbers to find meaningful connections. Combining them, we create a semantic search engine that's both smart and swift.\n",
    "\n",
    "<!-- Insert a diagram that visually represents the flow from text input to vectorization with USE to searching and indexing with FAISS -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a9016-26ca-4a35-bed3-e003378c21d2",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b217ec-c213-48aa-862a-1ac51263a7e9",
   "metadata": {},
   "source": [
    "## The 20 Newsgroups Dataset\n",
    "\n",
    "In this project, we'll be using the 20 Newsgroups dataset, a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. It's a go-to dataset in the NLP community because it presents real-world challenges:\n",
    "\n",
    "### What is the 20 Newsgroups Dataset?\n",
    "\n",
    "- **Diverse Topics**: The dataset spans 20 different topics, from sports and science to politics and religion, reflecting the diverse interests of newsgroup members.\n",
    "- **Natural Language**: It contains actual discussions, with all the nuances of human language, making it ideal for semantic search.\n",
    "- **Prevalence of Context**: The conversations within it require understanding of context to differentiate between the topics effectively.\n",
    "\n",
    "### How are we using the 20 Newsgroups Dataset?\n",
    "\n",
    "1. **Exploring Data**: We'll start by loading the dataset and exploring its structure to understand the kind of information it holds.\n",
    "2. **Preprocessing**: We'll clean the text data, removing any unwanted noise that could affect our semantic analysis.\n",
    "3. **Vectorization**: We'll then use the Universal Sentence Encoder to transform this text into numerical vectors that capture the essence of each document.\n",
    "4. **Semantic Search Implementation**: Finally, we'll use FAISS to index these vectors, allowing us to perform fast and efficient semantic searches across the dataset.\n",
    "\n",
    "By working with the 20 Newsgroups dataset, you'll gain hands-on experience with real-world data and the end-to-end process of building a semantic search engine.\n",
    "\n",
    "<!-- An image of a sample newsgroup post or a chart showing the distribution of topics within the dataset can be helpful here -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4a78-b02a-421a-b817-e6d28fa410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec8512-dc09-4ac6-a412-dadee693fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fd320-cfef-43c0-aacb-41feb8c0578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 posts from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"Sample post {i+1}:\\n\")\n",
    "    pprint(newsgroups_train.data[i])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811a1ea-cfca-467c-bdf7-09d97ded06c6",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539eae39-f44f-40f4-9ea1-df0ee19ab5c2",
   "metadata": {},
   "source": [
    "# Pre-processing Data\n",
    "\n",
    "In this section, we focus on preparing the text data from the 20 Newsgroups dataset for our semantic search engine. Preprocessing is a critical step to ensure the quality and consistency of the data before it's fed into the Universal Sentence Encoder.\n",
    "\n",
    "## Steps in Preprocessing:\n",
    "\n",
    "1. **Fetching Data**: \n",
    "   - We load the complete 20 Newsgroups dataset using `fetch_20newsgroups` from `sklearn.datasets`. \n",
    "   - `documents = newsgroups.data` stores all the newsgroup documents in a list.\n",
    "\n",
    "2. **Defining the Preprocessing Function**:\n",
    "   - The `preprocess_text` function is designed to clean each text document. Here's what it does to every piece of text:\n",
    "     - **Removes Email Headers**: Strips off lines that start with 'From:' as they usually contain metadata like email addresses.\n",
    "     - **Eliminates Email Addresses**: Finds patterns resembling email addresses and removes them.\n",
    "     - **Strips Punctuations and Numbers**: Removes all characters except alphabets, aiding in focusing on textual data.\n",
    "     - **Converts to Lowercase**: Standardizes the text by converting all characters to lowercase, ensuring uniformity.\n",
    "     - **Trims Excess Whitespace**: Cleans up any extra spaces, tabs, or line breaks.\n",
    "\n",
    "3. **Applying Preprocessing**:\n",
    "   - We iterate over each document in the `documents` list and apply our `preprocess_text` function.\n",
    "   - The cleaned documents are stored in `processed_documents`, ready for further processing.\n",
    "\n",
    "By preprocessing the text data in this way, we reduce noise and standardize the text, which is essential for achieving meaningful semantic analysis in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432af00f-5b97-4335-82f5-7838769b773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "documents = newsgroups.data\n",
    "\n",
    "# Basic preprocessing of text data\n",
    "def preprocess_text(text):\n",
    "    # Remove email headers\n",
    "    text = re.sub(r'^From:.*\\n?', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess each document\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af3848-590e-472a-bdb1-4c5fdd336457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample post to display\n",
    "sample_index = 0  # for example, the first post in the dataset\n",
    "\n",
    "# Print the original post\n",
    "print(\"Original post:\\n\")\n",
    "print(newsgroups_train.data[sample_index])\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the preprocessed post\n",
    "print(\"Preprocessed post:\\n\")\n",
    "print(preprocess_text(newsgroups_train.data[sample_index]))\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f6c36-64e1-4d36-ba0e-12bafcaa5cbf",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275ca89-9a3d-4173-8e25-16b5505645c0",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder\n",
    "\n",
    "After preprocessing the text data, the next step is to transform this cleaned text into numerical vectors using the Universal Sentence Encoder (USE). These vectors capture the semantic essence of the text.\n",
    "\n",
    "### Loading the USE Module:\n",
    "\n",
    "- We use TensorFlow Hub (`hub`) to load the pre-trained Universal Sentence Encoder.\n",
    "- `embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")` fetches the USE module, making it ready for vectorization.\n",
    "\n",
    "### Defining the Embedding Function:\n",
    "\n",
    "- The `embed_text` function is defined to take a piece of text as input and return its vector representation.\n",
    "- Inside the function, `embed(text)` converts the text into a high-dimensional vector, capturing the nuanced semantic meaning.\n",
    "- `.numpy()` is used to convert the result from a TensorFlow tensor to a NumPy array, which is a more versatile format for subsequent operations.\n",
    "\n",
    "### Vectorizing Preprocessed Documents:\n",
    "\n",
    "- We then apply the `embed_text` function to each document in our preprocessed dataset, `processed_documents`.\n",
    "- `np.vstack([...])` stacks the vectors vertically to create a 2D array, where each row represents a document.\n",
    "- The resulting array `X_use` holds the vectorized representations of all the preprocessed documents, ready to be used for semantic search indexing and querying.\n",
    "\n",
    "By vectorizing the text with USE, we've now converted our textual data into a format that can be efficiently processed by machine learning algorithms, setting the stage for the next step: indexing with FAISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2de3b3-541e-4270-be07-2ef134b49138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def embed_text(text):\n",
    "    return embed(text).numpy()\n",
    "\n",
    "# Generate embeddings for each preprocessed document\n",
    "X_use = np.vstack([embed_text([doc]) for doc in processed_documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daeea34-77ac-4e21-b56e-c6e849c7ab3e",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94a5b5-c4d1-4b18-80e7-3bda09d7910c",
   "metadata": {},
   "source": [
    "# Indexing with FAISS\n",
    "\n",
    "With our documents now represented as vectors using the Universal Sentence Encoder, the next step is to use FAISS (Facebook AI Similarity Search) for efficient similarity searching.\n",
    "\n",
    "## Creating a FAISS Index:\n",
    "\n",
    "- We first determine the dimension of our vectors from `X_use` using `X_use.shape[1]`.\n",
    "- A FAISS index (`index`) is created specifically for L2 distance (Euclidean distance) using `faiss.IndexFlatL2(dimension)`.\n",
    "- We add our document vectors to this index with `index.add(X_use)`. This step effectively creates a searchable space for our document vectors.\n",
    "\n",
    "### Choosing the Right Index:\n",
    "\n",
    "- In this project, we use `IndexFlatL2` for its simplicity and effectiveness in handling small to medium-sized datasets.\n",
    "- FAISS offers a variety of indexes tailored for different use cases and dataset sizes. Depending on your specific needs and the complexity of your data, you might consider other indexes for more efficient searching.\n",
    "- For larger datasets or more advanced use cases, indexes like `IndexIVFFlat`, `IndexIVFPQ`, and others can provide faster search times and reduced memory usage. Explore more at [FAISS indexes wiki](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2baf25d-7fdc-4ab5-9607-86e732efa825",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = X_use.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # Creating a FAISS index\n",
    "index.add(X_use)  # Adding the document vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faef8c9-92fa-4fbf-83b5-58532cc4cab0",
   "metadata": {},
   "source": [
    "##  Quering with FAISS\n",
    "### Defining the Search Function:\n",
    "\n",
    "- The `search` function is designed to find documents that are semantically similar to a given query.\n",
    "- It preprocesses the query text using the `preprocess_text` function to ensure consistency.\n",
    "- The query text is then converted to a vector using `embed_text`.\n",
    "- FAISS performs a search for the nearest neighbors (`k`) to this query vector in our index.\n",
    "- It returns the distances and indices of these nearest neighbors.\n",
    "\n",
    "### Executing a Query and Displaying Results:\n",
    "\n",
    "- We test our search engine with an example query (e.g., \"motorcycle\").\n",
    "- The `search` function returns the indices of the documents in the index that are most similar to the query.\n",
    "- For each result, we display:\n",
    "   - The ranking of the result (based on distance).\n",
    "   - The distance value itself, indicating how close the document is to the query.\n",
    "   - The actual text of the document. We display both the preprocessed and original versions of each document for comparison.\n",
    "\n",
    "This functionality showcases the practical application of semantic search: retrieving information that is contextually relevant to the query, not just based on keyword matching. The displayed results will give a clear idea of how our semantic search engine interprets and responds to natural language queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd42dd-e345-4836-a7e9-ef18f1df6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a query using the Faiss index\n",
    "def search(query_text, k=5):\n",
    "    # Preprocess the query text\n",
    "    preprocessed_query = preprocess_text(query_text)\n",
    "    # Generate the query vector\n",
    "    query_vector = embed_text([preprocessed_query])\n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_vector.astype('float32'), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example Query\n",
    "query_text = \"motorcycle\"\n",
    "distances, indices = search(query_text)\n",
    "\n",
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Ensure that the displayed document is the preprocessed one\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{processed_documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6780d-87c8-4956-8939-0b8ac430c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Displaying the original (unprocessed) document corresponding to the search result\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446d6df-dc4e-4717-9683-76c4571bba97",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c185cb-2074-479c-8492-6943f27ea1fc",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf0a73-8bbf-4847-a006-2b2d0fef689e",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb4a34-e2fc-45ef-a95b-3df9114e5fa5",
   "metadata": {},
   "source": [
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e88a3-160a-49d7-ae89-03fa1e2217b3",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for the changelog</summary>\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-01-08|0.1|Ashutosh Sagar|SME initial creation|\n",
    "|2025-07-17|0.2|Steve Ryan|ID review and format fixes|\n",
    "|2025-07-25|0.3|Steve Ryan|ID fixed TOC and lab title|\n",
    "\n",
    "</detials>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbf712-4734-4ad0-87e7-66007fc0a63b",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "prev_pub_hash": "faf67be6dc53caed3c38c6e488c2b44148930d55cdd4791ae555934f1d44b563"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
